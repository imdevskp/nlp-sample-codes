{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize, RegexpTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_text = \"\"\"Hi Mr. Smith! I’m going to buy some vegetables (tomatoes and cucumbers) from the store. \n",
    "Should I pick up some black-eyed peas as well?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi',\n",
       " 'Mr.',\n",
       " 'Smith',\n",
       " '!',\n",
       " 'I',\n",
       " '’',\n",
       " 'm',\n",
       " 'going',\n",
       " 'to',\n",
       " 'buy',\n",
       " 'some',\n",
       " 'vegetables',\n",
       " '(',\n",
       " 'tomatoes',\n",
       " 'and',\n",
       " 'cucumbers',\n",
       " ')',\n",
       " 'from',\n",
       " 'the',\n",
       " 'store',\n",
       " '.',\n",
       " 'Should',\n",
       " 'I',\n",
       " 'pick',\n",
       " 'up',\n",
       " 'some',\n",
       " 'black-eyed',\n",
       " 'peas',\n",
       " 'as',\n",
       " 'well',\n",
       " '?']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(my_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi Mr. Smith!',\n",
       " 'I’m going to buy some vegetables (tomatoes and cucumbers) from the store.',\n",
       " 'Should I pick up some black-eyed peas as well?']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(my_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular expression tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi',\n",
       " 'Mr.',\n",
       " 'Smith!',\n",
       " 'I’m',\n",
       " 'going',\n",
       " 'to',\n",
       " 'buy',\n",
       " 'some',\n",
       " 'vegetables',\n",
       " '(tomatoes',\n",
       " 'and',\n",
       " 'cucumbers)',\n",
       " 'from',\n",
       " 'the',\n",
       " 'store.',\n",
       " 'Should',\n",
       " 'I',\n",
       " 'pick',\n",
       " 'up',\n",
       " 'some',\n",
       " 'black-eyed',\n",
       " 'peas',\n",
       " 'as',\n",
       " 'well?']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whitespace_tokenizer = RegexpTokenizer(\"\\s+\", gaps=True)\n",
    "whitespace_tokenizer.tokenize(my_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi', 'Mr', 'Smith', 'Should']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap_tokenizer = RegexpTokenizer(\"[A-Z]['\\w]+\")\n",
    "cap_tokenizer.tokenize(my_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_words = word_tokenize(my_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hi',),\n",
       " ('Mr.',),\n",
       " ('Smith',),\n",
       " ('!',),\n",
       " ('I',),\n",
       " ('’',),\n",
       " ('m',),\n",
       " ('going',),\n",
       " ('to',),\n",
       " ('buy',),\n",
       " ('some',),\n",
       " ('vegetables',),\n",
       " ('(',),\n",
       " ('tomatoes',),\n",
       " ('and',),\n",
       " ('cucumbers',),\n",
       " (')',),\n",
       " ('from',),\n",
       " ('the',),\n",
       " ('store',),\n",
       " ('.',),\n",
       " ('Should',),\n",
       " ('I',),\n",
       " ('pick',),\n",
       " ('up',),\n",
       " ('some',),\n",
       " ('black-eyed',),\n",
       " ('peas',),\n",
       " ('as',),\n",
       " ('well',),\n",
       " ('?',)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unigrams\n",
    "list(ngrams(my_words, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hi', 'Mr.'),\n",
       " ('Mr.', 'Smith'),\n",
       " ('Smith', '!'),\n",
       " ('!', 'I'),\n",
       " ('I', '’'),\n",
       " ('’', 'm'),\n",
       " ('m', 'going'),\n",
       " ('going', 'to'),\n",
       " ('to', 'buy'),\n",
       " ('buy', 'some'),\n",
       " ('some', 'vegetables'),\n",
       " ('vegetables', '('),\n",
       " ('(', 'tomatoes'),\n",
       " ('tomatoes', 'and'),\n",
       " ('and', 'cucumbers'),\n",
       " ('cucumbers', ')'),\n",
       " (')', 'from'),\n",
       " ('from', 'the'),\n",
       " ('the', 'store'),\n",
       " ('store', '.'),\n",
       " ('.', 'Should'),\n",
       " ('Should', 'I'),\n",
       " ('I', 'pick'),\n",
       " ('pick', 'up'),\n",
       " ('up', 'some'),\n",
       " ('some', 'black-eyed'),\n",
       " ('black-eyed', 'peas'),\n",
       " ('peas', 'as'),\n",
       " ('as', 'well'),\n",
       " ('well', '?')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bigrams\n",
    "list(ngrams(my_words, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hi', 'Mr.', 'Smith'),\n",
       " ('Mr.', 'Smith', '!'),\n",
       " ('Smith', '!', 'I'),\n",
       " ('!', 'I', '’'),\n",
       " ('I', '’', 'm'),\n",
       " ('’', 'm', 'going'),\n",
       " ('m', 'going', 'to'),\n",
       " ('going', 'to', 'buy'),\n",
       " ('to', 'buy', 'some'),\n",
       " ('buy', 'some', 'vegetables'),\n",
       " ('some', 'vegetables', '('),\n",
       " ('vegetables', '(', 'tomatoes'),\n",
       " ('(', 'tomatoes', 'and'),\n",
       " ('tomatoes', 'and', 'cucumbers'),\n",
       " ('and', 'cucumbers', ')'),\n",
       " ('cucumbers', ')', 'from'),\n",
       " (')', 'from', 'the'),\n",
       " ('from', 'the', 'store'),\n",
       " ('the', 'store', '.'),\n",
       " ('store', '.', 'Should'),\n",
       " ('.', 'Should', 'I'),\n",
       " ('Should', 'I', 'pick'),\n",
       " ('I', 'pick', 'up'),\n",
       " ('pick', 'up', 'some'),\n",
       " ('up', 'some', 'black-eyed'),\n",
       " ('some', 'black-eyed', 'peas'),\n",
       " ('black-eyed', 'peas', 'as'),\n",
       " ('peas', 'as', 'well'),\n",
       " ('as', 'well', '?')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trigrams\n",
    "list(ngrams(my_words, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "* Stemming normalize words into its base form or root form\n",
    "* It doesn't promise to output a proper word\n",
    "* **Porter stemmer** usually removes prefix and affixes, whereas \n",
    "* **Lancaster stemmer** is more aggressive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"give\", \"giving\", \"given\", \"gave\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give \t:  give\n",
      "giving \t:  give\n",
      "given \t:  given\n",
      "gave \t:  gave\n"
     ]
    }
   ],
   "source": [
    "# porter stemmer\n",
    "ps = PorterStemmer()\n",
    "for i in words:\n",
    "    print(i, '\\t: ', ps.stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give \t:  giv\n",
      "giving \t:  giv\n",
      "given \t:  giv\n",
      "gave \t:  gav\n"
     ]
    }
   ],
   "source": [
    "# lancaster stemmer\n",
    "ls = LancasterStemmer()\n",
    "for i in words:\n",
    "    print(i, '\\t: ', ls.stem(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "* Lemmatization is similar to stemming\n",
    "* It maps words to into one common root\n",
    "* Difference is that output of lemmatization is a \"PROPER WORD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give \t:  give\n",
      "giving \t:  give\n",
      "given \t:  give\n",
      "gave \t:  give\n"
     ]
    }
   ],
   "source": [
    "wl = WordNetLemmatizer()\n",
    "\n",
    "for i in words:\n",
    "    print(i, '\\t: ', wl.lemmatize(i, pos='v')) # pos = part of speech , v = verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method WordNetLemmatizer.lemmatize of <WordNetLemmatizer>>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wl.lemmatize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS (Parts of Speech)\n",
    "* Parts of speech tagging labels each word as a part of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"In New York, I like to ride the Metro to visit MOMA ans some restaurants rated well by Ruth Reichl.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(text)\n",
    "tag_text = pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('James', 'NNP'),\n",
       " ('Smith', 'NNP'),\n",
       " ('lives', 'VBZ'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('United', 'NNP'),\n",
       " ('States', 'NNPS'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_text = \"James Smith lives in the United States.\"\n",
    "pos_tag(word_tokenize(my_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER (Name Entity Recognition)\n",
    "* Identifies and tags named entities in text like\n",
    "> * people\n",
    "> * places\n",
    "> * organizations\n",
    "> * phone numbers\n",
    "> * emails\n",
    "* Can be tremendously valuable for further NLP tasks\n",
    "* For example: “United States” --> “United_States”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.chunk import ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_text = \"James Smith lives in the United States.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('James', 'NNP'),\n",
       " ('Smith', 'NNP'),\n",
       " ('lives', 'VBZ'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('United', 'NNP'),\n",
       " ('States', 'NNPS'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label each word as a part of speech\n",
    "tokens = pos_tag(word_tokenize(my_text)) \n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The Ghostscript executable isn't found.\n",
      "See http://web.mit.edu/ghostscript/www/Install.htm\n",
      "If you're using a Mac, you can try installing\n",
      "https://docs.brew.sh/Installation then `brew install ghostscript`\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tree.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    805\u001b[0m                             \u001b[0menv_vars\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'PATH'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 806\u001b[1;33m                             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    807\u001b[0m                         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_binary\u001b[1;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[0;32m    696\u001b[0m         find_binary_iter(\n\u001b[1;32m--> 697\u001b[1;33m             \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_to_bin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearchpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    698\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_binary_iter\u001b[1;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[0;32m    680\u001b[0m     for file in find_file_iter(\n\u001b[1;32m--> 681\u001b[1;33m         \u001b[0mpath_to_bin\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearchpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    682\u001b[0m     ):\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_file_iter\u001b[1;34m(filename, env_vars, searchpath, file_names, url, verbose, finding_dir)\u001b[0m\n\u001b[0;32m    638\u001b[0m         \u001b[0mdiv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'='\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m75\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 639\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n\\n%s\\n%s\\n%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n\n===========================================================================\nNLTK was unable to find the gs file!\nUse software specific configuration paramaters or set the PATH environment variable.\n===========================================================================",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tree.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    817\u001b[0m                                         \"https://docs.brew.sh/Installation then `brew install ghostscript`\")                \n\u001b[0;32m    818\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpre_error_message\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tree('S', [Tree('PERSON', [('James', 'NNP')]), Tree('PERSON', [('Smith', 'NNP')]), ('lives', 'VBZ'), ('in', 'IN'), ('the', 'DT'), Tree('GPE', [('United', 'NNP'), ('States', 'NNPS')]), ('.', '.')])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # extract entities from the list of words\n",
    "# entities = ne_chunk(tokens) \n",
    "# entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compound Term Extraction\n",
    "* This can be very valuable for special cases\n",
    "* For example: “black eyed peas“ --> “black_eyed_peas”\n",
    "* This totally changes the conceptual meaning!\n",
    "* Named entity recognition groups together words and identifies entities,\n",
    "* But doesn’t capture them all, so you can identify your own compound words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import MWETokenizer # multi-word expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_text = \"You all are the greatest students of all time.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['You_all', 'are', 'the', 'greatest', 'students', 'of_all_time', '.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mwe_tokenizer = MWETokenizer([('You','all'), ('of', 'all', 'time')])\n",
    "mwe_tokens = mwe_tokenizer.tokenize(word_tokenize(my_text))\n",
    "mwe_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cat is in the box. The cat likes the box. The box is over the cat.\n"
     ]
    }
   ],
   "source": [
    "# text\n",
    "text = \"The cat is in the box. The cat likes the box. The box is over the cat.\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to lowercase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the cat is in the box. the cat likes the box. the box is over the cat.\n"
     ]
    }
   ],
   "source": [
    "text_lower = text.lower()\n",
    "print(text_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'cat', 'is', 'in', 'the', 'box', '.', 'the', 'cat', 'likes', 'the', 'box', '.', 'the', 'box', 'is', 'over', 'the', 'cat', '.']\n"
     ]
    }
   ],
   "source": [
    "# tokeinzing words\n",
    "tokens = word_tokenize(text_lower)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### removing non alpha characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'cat', 'is', 'in', 'the', 'box', 'the', 'cat', 'likes', 'the', 'box', 'the', 'box', 'is', 'over', 'the', 'cat']\n"
     ]
    }
   ],
   "source": [
    "# choosing only words omiting punctuations, numbers ...\n",
    "tokens  = [i for i in tokens if i.isalpha()]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'box', 'cat', 'likes', 'box', 'box', 'cat']\n"
     ]
    }
   ],
   "source": [
    "not_stop_words = [i for i in tokens if i not in stopwords.words('english')]\n",
    "print(not_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'cat': 3, 'box': 3, 'likes': 1})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count words\n",
    "Counter(not_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words / Word counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'The': 3,\n",
       "         'cat': 3,\n",
       "         'is': 2,\n",
       "         'in': 1,\n",
       "         'the': 3,\n",
       "         'box': 3,\n",
       "         '.': 3,\n",
       "         'likes': 1,\n",
       "         'over': 1})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = \"The cat is in the box. The cat likes the box. The box is over the cat.\"\n",
    "Counter(word_tokenize(sent)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting word length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2., 0., 1., 0., 0., 0., 3., 0., 0., 1.]),\n",
       " array([1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5, 5. , 5.5, 6. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "words = word_tokenize(\"This is a pretty cool tool!\")\n",
    "word_lengths = [len(w) for w in words]\n",
    "plt.hist(word_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.which(\"gs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
